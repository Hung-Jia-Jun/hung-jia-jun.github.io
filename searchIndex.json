[
{
		"title": "2025-12-17",
		"date":"Wed Dec 17 2025 00:00:00 GMT+0000 (Coordinated Universal Time)",
		"url":"/daily-note/2025-12-17/",
		"content": "Target\nNote",
		"tags": [ "note","Daily_note"]
},

{
		"title": "Consumer offset reset 行為",
		"date":"Wed Dec 17 2025 14:27:46 GMT+0000 (Coordinated Universal Time)",
		"url":"/apache-kafka/consumer-offset-reset/",
		"content": "情境\n\nconsumer 預期會從 kafka 持續讀取 log，但如果 consumer crash，kafka 會保存 commited offset 7 天\n也說明，若 consumer 停機超過 7 天，之前消費的位置將會被重置\n參數\n\nauto.offset.reset=latest: 會從 kafka topic 最末端讀取 log\nauto.offset.reset=earliest: 從最早的地方開始讀 log\nauto.offset.reset=none: 若沒有 offset 資訊，將會拋出 exception\n\n重播 log 給 consumer\n\n步驟如下:\n\n關閉該 consumer group 底下所有的 consumer\n使用 kafka-consumer-groups 重置你想重置的 offset 位置\n重啟 consumer\n\nJava code\n透過 addShutdownHook 偵測 shutdown event\n...\n// get a reference\nfinal Thread mainThread = Thread.currentThread();\n\nRuntime.getRuntime().addShutdownHook(new Thread(){\npublic void run(){\nlog.info(&quot;Detected a shutdown event&quot;);\nconsumer.wakeup();\n\ntry {\nmainThread.join();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n});\n...\ntry(openSearchClient; consumer){\nboolean indexExists = openSearchClient.indices().exists(new GetIndexRequest(indexName), RequestOptions.DEFAULT);\nif (!indexExists){\n// we need to create the index on opensearch if it doesn't exist already\nCreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName);\nopenSearchClient.indices().create(createIndexRequest, RequestOptions.DEFAULT);\nlog.info(&quot;The wikimedia index has been created&quot;);\n} else {\nlog.info(&quot;The wikimedia index already exists&quot;);\n}\n\nwhile (true){\nConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(3000));\n\nint recordCount = records.count();\nlog.info(&quot;Received &quot; + recordCount + &quot; record(s)&quot;);\nBulkRequest bulkRequest = new BulkRequest();\nfor (ConsumerRecord&lt;String, String&gt; record : records){\ntry{\nString id = extractId(record.value());\n// send the record into opensearch\nIndexRequest indexRequest = new IndexRequest(indexName)\n.source(record.value(), XContentType.JSON)\n.id(id);\n//IndexResponse indexResponse = openSearchClient.index(indexRequest, RequestOptions.DEFAULT);\nbulkRequest.add(indexRequest);\n} catch (Exception e){\n\n}\n}\nif (bulkRequest.numberOfActions() &gt; 0){\nBulkResponse bulkResponse = openSearchClient.bulk(bulkRequest, RequestOptions.DEFAULT);\nlog.info(&quot;Inserted &quot; + bulkResponse.getItems().length + &quot; record(s).&quot;);\ntry {\nThread.sleep(1000);\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n\n}\n\nconsumer.commitAsync();\nlog.info(&quot;offset have been committed!&quot;);\n}\n} catch (WakeupException e){\nlog.info(&quot;Consumer is starting to shutdown&quot;);\n} catch (Exception e){\nlog.error(&quot;unexpected exception: &quot;, e);\n} finally {\nconsumer.close(); // close the consumer, this will also commit offest to kafka.\nopenSearchClient.close();\nlog.info(&quot;The consumer is now gracefully shut down&quot;);\n}",
		"tags": [ "note","kafka","java"]
},

{
		"title": "kafka 高效率傳輸設定",
		"date":"Wed Dec 17 2025 14:27:46 GMT+0000 (Coordinated Universal Time)",
		"url":"/",
		"content": "訂閱 kafka 消息\nkafka-console-consumer --bootstrap-server 127.0.0.1:19092 --topic wikimedia.recentchange\n\n高效率傳輸時，可考慮以下設定\n\nmax.in.flight.requests.per.connection\n\n每個 producer 在 broker 回覆 ack 前，最多送幾筆訊息出去\n若設定 = 1\n\n訊息只會一筆一筆發，會降低效率，但好處是，若訊息需要嚴格的排序(有新增 sort key)，那很重要\n\nlinger.ms\n\n等待一段 linger.ms，在此期間收到的消息都放在自己的暫存區，若 broker 批處理(batch.size)在 linger.ms 到達之前填滿，則立即批處理暫存區內的訊息，否則達到 linger.ms 才進行批處理\n\ncompression.type\n\n批處理參數，用於 broker 端壓縮訊息使用的算法(e.g. lz4、zstd、gzip...etc)\n\nbatch.size\n\n批處理的單筆 message 大小，若超過，則立即處理該訊息",
		"tags": [ "note","kafka","gardenEntry"]
}
]